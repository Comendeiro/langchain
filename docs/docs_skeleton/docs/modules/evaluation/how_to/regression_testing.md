# Regression Testing

When dealing with model API's, it can be hard to know if the prediction quality has changed without proper regression testing. This guide will touch on three easy ways
to regression test your model API's. We will use a QA system as an example. They all depend on constructing a dataset of inputs. It's best for inputs to be representative of your application domain.

**Important:** As with any system, it's important to isolate what you want to test. If you are regression testing an LLM API, test it directly or mock other components of your application.

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->


```python
from langchain.evaluation.loading import load_dataset
```


```python
inputs = []
```


## Approach 1: Compare Aggregate Performance

The first approach is to construct an example dataset with reference examples. You can test the accuracy (or other metrics) of your model on a schedule to ensure the accuracy of your model is not degrading.


```python
from langchain.chat_models import  ChatOpenAI
llm = ChatOpenAI(model="gpt-3.5-turbo-0631", temperature=0)
# TODO
```

## Approach 2: Pairwise Compare Outputs

The second way you can track changes and regressions is to compare outputs of the model on identical inputs. You can use a simple exact (or fuzzy) string match metric
or use a model graded metric to ensure the meanings of the outputs are the same.



```python
# TODO
```
