{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da95378",
   "metadata": {},
   "source": [
    "# Pairwise String Comparison\n",
    "\n",
    "Often you will want to compare predictions of an LLM, Chain, or Agent for a given input. The comparison evaluators facilitate this so you can answer questions like:\n",
    "- Which LLM or Prompt produces a preferred output for a given question?\n",
    "- Which completions should I include for few-shot example selection?\n",
    "- Which output is better to include for fintetuning?\n",
    "\n",
    "You can use the PairwiseStringEvalChain to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6790c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import PairwiseStringEvalChain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "\n",
    "eval_chain = PairwiseStringEvalChain.from_llm(llm=llm, requires_reference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ad9139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Response A provides an incorrect answer by stating there are three dogs in the park, while the reference answer indicates there are four. Response B, on the other hand, provides the correct answer, matching the reference. Although Response B is less detailed, it is accurate and directly answers the question. \\n\\nTherefore, the better response is [[B]].\\n',\n",
       " 'value': 'B',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_chain.evaluate_string_pairs(\n",
    "    prediction = \"there are three dogs\",\n",
    "    prediction_b=\"4\",\n",
    "    input=\"how many dogs are in the park?\",\n",
    "    reference=\"four\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed353b93-be71-4479-b9c0-8c97814c2e58",
   "metadata": {},
   "source": [
    "## Without References\n",
    "\n",
    "When references aren't available, you can still predict the preferred response.\n",
    "The results will reflect the evaluation model's preference, which is less reliable and may result\n",
    "in preferences that are factually incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "586320da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_chain = PairwiseStringEvalChain.from_llm(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f56c76e-a39b-4509-8b8a-8a2afe6c3da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Both responses answer the question directly and accurately, but neither provides any additional detail or context. Response A is slightly more complete because it uses a full sentence, while Response B only provides a number. However, both responses are relevant and accurate, so the difference is minimal.\\n\\nFinal decision: [[C]]\\n',\n",
       " 'value': None,\n",
       " 'score': 0.5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_chain.evaluate_string_pairs(\n",
    "    prediction = \"there are three dogs\",\n",
    "    prediction_b=\"4\",\n",
    "    input=\"What is the name of the dog?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84a958-1330-482b-b950-68bcf23f9e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
